---
layout: post
title:  "Linux 0.12源码阅读之进程调度"
date:   2015-05-12 20:33:25
categories: linux 
tags: linux
---

#	1.80X86的多任务管理体系

Linux 0.12只支持80X86体系结构，非常依赖X86提供的多任务管理体系，X86体系结构中最为重要的是内存的分段和分页，中断和异常处理，以及构建在这个基础上的保护机制。首先来看X86体系结构中几个非常重要的概念：

*	段描述符：描述了一个内存段, 具体的是BASE Address + Limit + 其他信息
*	段选择符：选择了一个段描述符，具体的是段描述符index + TI+ RPL(TI: 0 GDT/1 LDT   RPL:权限保护)。
*	IDT描述符: 一个中断向量对应的处理函数代码，具体的是段选择符 + offset +其他信息

80X86提供了GDTR, LDTR, IDTR, TR等用来做多任务管理的寄存器.
![](/assets/2015-05-12-linux012-kernel/segment.png)

*	GDTR(32 bits BASE + 16 bits limit)用来选择GDT,
*	LDTR(16 bits + 32 bits hidden)用来选择GDT中的某一项LDT
*	IDTR(32 bits BASE + 16 bits limit)用来选择IDT
*	TR(16 bits + 32 bits hidden)用来选择GDT中的某一项TSS

GDT里面的信息每一条是一个段(segment)，可以有CS, DS, TSS, LDT等。LDT里面的信息每一条页是一个段(segment)，不过只可以有CS, DS。
BASE代表的是一个32bit线性地址。GDT/LDT将内存分段。至于线性地址到物理地址的映射，由分页机制完成。Linux在初始化内存时，将物理内存16M一共1024\*4个页面，一一映射到pg_dir 和pg0,pg1,pg2,pg3中。也就是说，初始化完成了线性地址空间0-16M到物理地址的映射。所以在GDT中写入的物理地址作为BASE，可以通过查找页目录表和页表，定位到具体的物理地址。  

GDT CS/DS的BASE=0， GDT LDT/TSS的BASE为4M-16M的线性地址(==物理地址)。LDT 的BASE为线性地址 nr\*64M，映射到4M-16M的物理地址(+swap)。而nr\*64M的线性地址到物理地址的映射并不在初始化之内，所以任务初始化时需要将线性地址nr*64M到实际的物理地址映射的存储(即页表pgX)生成，并加到pg_dir中。而后续的线性地址到物理内存之间的映射，包括页表在内存中得生成等是由内存管理模块完成的。 

IDT 里面的每一条代表的是程序入口(segment:offset)，其中segment index必须是GDT里面的。IDT里面的中断描述符有3种 

*	interupte_gate dpl:00(仅对内核态开放) type:01110(interrupt gate IF=0)  
	_0x20 timer_interrupt_  
	_0x24 rs1_interrupt_  
	_0x23 rs2_interupt_  
*	trap_gate dpl:00(仅对内核态开放) type:01111 (trap gate IF=1)  
*	system_gate dpl:11(用户态开放) type:01111 (trap gate IF=1)   
	_3 int3_  
	_4 overflow_  
	_5 bounds_  
	_0x80  system_call_  

IF=0表示中断屏蔽，意味着当前中断处理服务是一个原子操作，不允许别的任务抢占CPU。时钟片的任务切换（timer_interrupt)就是这样的一个中断，所以每次一定独占的将CPU调度成功。
而系统调用则可能会被中断掉，典型的是系统调用被时间片中断掉，但是Linux2.4之前都是内核非抢占的，陷于system_call的内核态系统调用不会被调度出去。

linux 中system_gate trap_gate intr_gate的segment=0x0008 (kernel CS段)，偏移量是&idt[n] 
下图是段描述符
![](/assets/2015-05-12-linux012-kernel/segment2.png)

#	2.任务切换
任务切换是通过 ljmp _TSS(nr)，即跳转到TSS nr所在的GDT描述符。

##	2.1.用户态和内核态
分成两种情况讨论：

1.若当前程序处于用户态 用户进程正在运行并且没有陷于内核态，被动等待时钟切片用完，时钟中断程序执行。这时候会有用户态-内核态切换，用户态的上下文SS:ESP, EFLAGES, CS:EIP被cpu执行int时自动的保存到内核栈上，同时在任务切换时，该任务的内核态SS:ESP，当前任务内核CS: EIP(ljmp后面的指令地址)被保存到TSS中。一旦该任务被CPU重新调度到，则会从内核代码ljmp后面继续执行，一直到从中断调用处离开iret回到用户态执行。 

2.若当前程序处于内核态 比如说用户进程通过系统调用已进入内核态

*	运行在内核态的代码主动的进行休眠或者任务切换，通过调用sleep或switch_to (another)，当前任务的内核态SS:ESP, 当前任务的内核CS: EIP(ljmp后面的指令地址)被保留到TSS中。 一旦该任务被唤醒从而被重新调度到，则会从内核代码ljmp后面继续执行，也即继续执行该任务的内核态代码(某个系统调用)。 
*	陷入内核态以后，被动等待时钟切片用完，时钟中断程序执行。在do_timer中判断被中断的任务运行在内核态，不会抢占该任务。所以得以从中断返回，继续执行该任务的内核态代码(某个系统调用)。 

这里值得注意的是，处于内核态的任务是不可以被抢占的，这一点使得内核栈不会无限的增大，最多有一个用户态-内核态切换的用户态栈保存，然后是中断调用需要保存的一些寄存器值。除非该任务在内核代码部分自己通过调用sleep将CPU时间让给别的任务。这一点是linux2.4之前非抢占式内核的一个简化设计。（Linux Kernel Development一书）

	//0x80系统中断发生时内核栈 
	old SS 
	old ESP 
	old EFLAGS 
	old CS 
	old EIP                  —int  iret 
	DS                         — system_call 
	ES 
	FS 
	-1 
	EDX 
	ECX 
	EBX 
	next EIP 
	call _sys_xxx 
	EAX   （系统调用号) 
	jmp ret_from_sys_call 

	//0x20系统中断发生时内核栈 
	old SS 
	old ESP 
	old EFLAGS 
	old CS 
	old EIP                  —int  iret 
	DS                        — system_interrupt 
	ES 
	FS 
	-1 
	EDX 
	ECX 
	EBX 
	EAX   （系统调用号) 
	CPL 
	call _do_timer  
	addl $4,%esp   // 
	jmp ret_from_sys_call 

##	2.2.任务切换的两种方式
前面已经讨论到了，主要有两种任务切换的方式

*	系统时钟中断，时间片到了以后对任务进行切换。这时候用户态-内核态(时钟中断)切换。其他类型的中断处理不会引起任务的切换，比如磁盘控制器发起的磁盘读写请求结束的中断，中断程序还是会在当前被中断任务的内核态执行。
*	当前任务主动sleep，把cpu时间让给其他进程。自己则等待某个特定事件发生以后，由别的任务唤醒自己，然后调度程序调度CPU到该任务继续执行。一般都是通过系统调用完成此类切换。 


###	2.2.1.系统时钟的例子
这是由timer时钟驱动，时钟中断自动完成的。

	sched.c     set_intr_gate(0x20,&timer_interrupt); 
	.align 2 
	_timer_interrupt: 
	    push %ds        # save ds,es and put kernel data space 
	    push %es        # into them. %fs is used by _system_call 
	    push %fs 
	    pushl $-1        # fill in -1 for orig_eax 
	    pushl %edx        # we save %eax,%ecx,%edx as gcc doesn not 
	    pushl %ecx        # save those across function calls. %ebx 
	    pushl %ebx        # is saved as we use that in ret_sys_call 
	    pushl %eax 
	    movl $0x10,%eax 
	    mov %ax,%ds 
	    mov %ax,%es 
	    movl $0x17,%eax 
	    mov %ax,%fs 
	    incl _jiffies 
	    movb $0x20,%al        # EOI to interrupt controller #1 
	    outb %al,$0x20 
	    movl CS(%esp),%eax 
	    andl $3,%eax        # %eax is CPL (0 or 3, 0=supervisor) 
	    pushl %eax 
	    call _do_timer        # 'do_timer(long CPL)' does everything from 
	    addl $4,%esp        # task switching to accounting ... 
	    jmp ret_from_sys_call 

	ret_from_sys_call: 
	    movl _current,%eax 
	    cmpl _task,%eax            # task[0] cannot have signals 
	    je 3f 
	    cmpw $0x0f,CS(%esp)        # was old code segment supervisor ? 
	    jne 3f 
	    cmpw $0x17,OLDSS(%esp)        # was stack segment = 0x17 ? 
	    jne 3f 
	    movl signal(%eax),%ebx 
	    movl blocked(%eax),%ecx 
	    notl %ecx 
	    andl %ebx,%ecx 
	    bsfl %ecx,%ecx 
	    je 3f 
	    btrl %ecx,%ebx 
	    movl %ebx,signal(%eax) 
	    incl %ecx 
	    pushl %ecx 
	    call _do_signal 
	    popl %ecx 
	    testl %eax, %eax 
	    jne 2b        # see if we need to switch tasks, or do more signals 
	3:    popl %eax 
	    popl %ebx 
	    popl %ecx 
	    popl %edx 
	    addl $4, %esp    # skip orig_eax 
	    pop %fs 
	    pop %es 
	    pop %ds 
	    iret           #从中断调用处回到原来被中断的指令地址，被中断的任务得以继续执行 


	//时钟调度中断执行程序，传入的值为当前被中断进程的cpl, cpl=0表示被中断进程运行在内核态，
	//cpl=3表示被中断程序运行在用户态
	void do_timer(long cpl)
	{
		static int blanked = 0;

		if (blankcount || !blankinterval) {
			if (blanked)
				unblank_screen();
			if (blankcount)
				blankcount--;
			blanked = 0;
		} else if (!blanked) {
			blank_screen();
			blanked = 1;
		}
		if (hd_timeout)
			if (!--hd_timeout)
				hd_times_out();

		if (beepcount)
			if (!--beepcount)
				sysbeepstop();

		if (cpl)
			current->utime++;
		else
			current->stime++;

		if (next_timer) {
			next_timer->jiffies--;
			while (next_timer && next_timer->jiffies <= 0) {
				void (*fn)(void);
				
				fn = next_timer->fn;
				next_timer->fn = NULL;
				next_timer = next_timer->next;
				(fn)();
			}
		}
		if (current_DOR & 0xf0)
			do_floppy_timer();
		if ((--current->counter)>0) return;
		current->counter=0;
		if (!cpl) return;
		schedule();
	}


###	2.2.2.高速缓存任务切换的例子：buffer_head 
*	任务A  --sleep  (buffer_head上的等待对列,由于buffer内存大小有限，太多的读写时后面的必须排队等待) 

```C
	if (!bh) {
		sleep_on(&buffer_wait);
		goto repeat;
	}
```
*	任务B释放缓存块，该队列上等待的任务可以继续执行  则wakeup该队列上的等待任务 

```C
//释放该块缓存，实际上数据仍然有效 (仍在hash queue中)。等到下一次读请求到底时，由于该buffer已经count置0并且解锁，是一个可以使用的
//缓存块，一旦该缓存块被选中(通过LRU算法，该缓存块最近最少使用)，则会从hash queue中删除表示该数据无效，
//并且读入新的数据，重新加入hash queue和free list中
void brelse(struct buffer_head * buf)
{
	if (!buf)
		return;
	wait_on_buffer(buf);
	if (!(buf->b_count--))
		panic("Trying to free free buffer");
	wake_up(&buffer_wait);
}
```


###	2.2.3.读写磁盘任务切换的例子：ll_rw_page
任务A需要读写某个block数据，将当前任务设为TASK_UNINTERRUPTIBLE，并将请求加入设备读写请求队列以后调用schedule，让出CPU时间。
磁盘读写不断中断调用(平均分布到所有任务上)，一旦任务A对应的磁盘读写完成，则内核唤醒该任务继续执行。

```C
*	任务A休眠
	//加入request需要等待的进程队列
	sleep_on(&wait_for_request);
	static inline void wait_on_buffer(struct buffer_head * bh)
	{
		cli();
		while (bh->b_lock)
			sleep_on(&bh->b_wait);
		sti();
	}
```
*	磁盘读写完成唤醒等待进程。

```C
	wake_up(&CURRENT->waiting);
	wake_up(&wait_for_request);
```

###	2.2.4.两个问题的思考
这点有两个问题需要澄清一下：

第一个问题是，当时钟周期达到，需要对任务进行切换，如果当前被中断的进程运行在user space，则会进行任务切换，切换以后，当前任务下一条执行的指令是被打断时候的用户进程的next EIP，还是当前时钟中断时执行任务调转ljmp的下一条语句？ 

	 /*	switch_to(n) should switch tasks to task nr n, first
	 * checking that n isn't the current task, in which case it does nothing.
	 * This also clears the TS-flag if the task we switched to has used
	 * tha math co-processor latest.
	 */
	 //任务切换，主要current定义在sched.c中
	 //ljmp 48mem [16 bit segment selector + 32 bits offset]
	 //16 bit segment selector即为_TSS(n)
	 //32 bits offset为__tmp.a
	#define switch_to(n) {\
	struct {long a,b;} __tmp; \
	__asm__("cmpl %%ecx,_current\n\t" \
		"je 1f\n\t" \
		"movw %%dx,%1\n\t" \
		"xchgl %%ecx,_current\n\t" \
		"ljmp %0\n\t" \                   //这句调用之后，cpu会执行其他任务的代码流
		"cmpl %%ecx,_last_task_used_math\n\t" \
		"jne 1f\n\t" \
		"clts\n" \
		"1:" \
		::"m" (*&__tmp.a),"m" (*&__tmp.b), \
		"d" (_TSS(n)),"c" ((long) task[n])); \
	}

我们知道，ljmp会进行TSS保存和切换，当前被切换任务的上下文(SS, ESP, CS,EIP, LDTR,CR3等)都被如数保存到TSS中，当前正在运行的CS为0x0001为内核代码段，EIP显然是ljmp的下一条指令，即cmpl %%ecx,_last_task_used_math。那么当前被中断任务重新调度成功后，又如何回到原来的被中断指令处继续执行呢？ 

在int指令调用前，当前被中断任务的SS:ESP(用户态堆栈), EFLAGS, CS:EIP(用户态代码及下一条执行指令)被压入栈，从中断调用do_timer返回时iret指令又将这些值悉数重新加载，所以当前被中断任务在重新被调度成功之后，将会继续执行。 

第二个问题是，int中断调用会把被中断程序用户态的CS, EIP, EFLAGS, SS:ESP等压入内核栈，中断调用可能会被阻塞，例如读写文件，等待socket打开等，等到当前被中断程序重新被CPU调度到, TSS切换执行ljmp下一条指令时，内存布局有可能改变。如何保证执行逻辑的不变？

我的理解是，内存布局不会改变。改变的只可能是内存从RAM到swap分区的交换，本质上已分配正在使用的内存数据不会销毁。
内核堆栈是位于task_struct的顶端的，task_struct数据页是位于16M以内的线性地址空间，用户态线性地址空间位于线性地址64M\*nr~64M\*(nr+1)，包括代码段CS和SS:ESP等，注意这两个部分内存都是从主存中分配得到的，而且不会主动释放掉。对于活动的进程程序执行的逻辑是不会改变的。

#	3.进程的休眠(等待)和唤醒(继续执行)
上面提到了有效情况下，进程需要等待某个资源可用，在此之前需要将进程休眠以让出CPU执行时间片，等待某个资源可以用的时候，内核再唤醒该进程使之可以继续执行。这些策略主要是通过sleep_on, wake_up函数来达成的。

函数sleep_on(task_struct \*\*p)会根据不同的资源产生不同的任务等待队列。

注意这里传入的是一个指针地址，该指针变量存储的是队列头指针地址。\*p=head。这样不同的task_struct对象形成不同的任务等待队列。下图是一个任务队列示意图。
![](/assets/2015-05-12-linux012-kernel/queue.png)

sleep_on常见的例子在I/O比较多，多对应系统调用read, write等，比如有：

*	高速缓存区对应的某个block块的读写任务，控制对象在bh->b_wait，这种控制有两层含义，第一层相当于是多个进程对特定的block块的读写并发操作控制。如果已经有进程对该块数据申请了读写操作，另外的读进程显然需要等待完成后才能被调度到。第二层含义是所有对该块数据申请了读写操作的进程(包括第一个申请进程), 都必须等待慢吞吞的磁盘I/O把读写请求执行完,由I/O模块通知读写完成，所有等待I/O的进程得以继续往下执行。
*	高速缓存区由于内存大小限制，需要将请求排队，形成一个请求队列。队列长度由高速缓存区大小限制， 队列头指针为buffer_wait。 当高速缓存区数据都被占满，没有办法满足新的请求时，新的任务必须等待其他进程完成I/O以后才可以被CPU调度执行。在此之前，新任务必须休眠。这相当于使用队列进行限流操作。
*	i_node对应的读写任务 inode->i_wait
*	Super_block 对应的 sb->s_wait
*	Block dev本身控制的读写缓冲区队列大小。相当于硬盘的读写缓冲区，通过这个缓冲区控制对硬盘电机的压力，这个读写缓冲区对上承接高速缓存，文件系统i_node, super_block以及内存管理模块的swap。队列头为 wait_for_request。对于读操作，32为队列长度，而对应写操作，32*2/3为队列长度。下面是源码中的注释

```c
	#define NR_BLK_DEV	7
	/*
	 * NR_REQUEST is the number of entries in the request-queue.
	 * NOTE that writes may use only the low 2/3 of these: reads
	 * take precedence.
	 *
	 * 32 seems to be a reasonable number: enough to get some benefit
	 * from the elevator-mechanism, but not so much as to lock a lot of
	 * buffers when they are in the queue. 64 seems to be too many (easily
	 * long pauses in reading when heavy writing/syncing is going on)
	 */
	#define NR_REQUEST	32
```

下面讨论一下多个进程同时竞争相同的block数据的情形。假设有三个进程，进程1，进程2和进程3同时对一块block数据buffer_head申请读操作，进程1首先会调用lock_buffer将该block锁住。并把读请求加入到block drv的请求队列中。之后进程1会继续往下执行。

	static inline void lock_buffer(struct buffer_head * bh)
	{
		cli();
		while (bh->b_lock)
			sleep_on(&bh->b_wait);
		bh->b_lock=1;
		sti();
	}

同时进程1，进程2，进程3在申请读操作时分别从bread()中调到wait_on_buffer，形成一个等待bh->b_wait结束的等待队列。一旦该块数据被读出，则进程1，进程2和进程3如多米诺骨牌按顺序唤醒执行

	static inline void wait_on_buffer(struct buffer_head * bh)
	{
		cli();
		while (bh->b_lock)
			sleep_on(&bh->b_wait);
		sti();
	}

如前所述，进程1会将b_lock置成1，然后继续执行，进程1，进程2则会和进程3组成一个等待队列，后申请的进程3为队列头指针。如果cpu调度进程2，则根据sleep_on的算法进程2会被休眠；如果cpu调度进程3，则进程3执行，并唤醒进程2. 但是进程3在执行到while(bh->b_lock)时，发现该缓存数据仍然是lock的，进程3只好继续休眠，和进程2仍然组成一个等待队列。队列头仍然是进程3. 重复以上步骤，进程3和进程2在该缓存变为unlock之前不能有其他操作。

当位于block dev层次的该请求块读写结束时，会对CURRENT->bh请求队列, 当前块请求进程CURRENT->waiting队列，块请求队列wait_for_request等进行唤醒操作。

	/*结束块读写请求 唤醒等待在当前block rw上的任务*/
	extern inline void end_request(int uptodate)
	{
		DEVICE_OFF(CURRENT->dev);
		//解除block并唤醒等待该block的任务队列
		if (CURRENT->bh) {
			CURRENT->bh->b_uptodate = uptodate;
			unlock_buffer(CURRENT->bh);
		}
		if (!uptodate) {
			printk(DEVICE_NAME " I/O error\n\r");
			printk("dev %04x, block %d\n\r",CURRENT->dev,
				CURRENT->bh->b_blocknr);
		}
		//唤醒可能的swap等待任务，注意swap读写时会一直阻塞并且req.waiting= current。 
		wake_up(&CURRENT->waiting);
		//唤醒等待加入block dev请求队列的任务
		wake_up(&wait_for_request);
		CURRENT->dev = -1;
		CURRENT = CURRENT->next;
	}

对应于第一个唤醒`unlock_buffer`，bh->b_wait上的等待进程1，进程2和进程3将得以调度，从wait_on_buffer中得以往下继续执行。

	static inline void unlock_buffer(struct buffer_head * bh)
	{
		if (!bh->b_lock)
			printk("ll_rw_block.c: buffer not locked\n\r");
		bh->b_lock = 0;
		wake_up(&bh->b_wait);
	}

CURRENT->waiting上的等待进程应该是swap读写进程，也会被唤醒从而继续执行。
wait_for_request上的等待任务也可以获得一个block req对象句柄从而将请求加入读写请求队列。

读到这里，优先读者可能会问，swap有可能多个进程同时访问同一个swap page么，如果有怎么做到进程同步，类似bh->b_waiting的机制的？

我们知道，swap_in函数是用来从swap分区读取对应的一页数据，swap_in函数唯一的使用情形是内存缺页。当CPU访问不在内存中的线性地址时，会产生一个中断异常，进一步进入中断处理程序调用do_no_page，如果此时发现该页已经初始化过并被交换到硬盘swap分区，则调用swap_in并且等待(block by cpu schedule)。这里swap_in针对的是线性地址(虚拟地址)，不同的进程分配的是不同的线性地址范围, 64M\*nr~64M\*(nr+1)，所以多个不同的进程被缺页中断时，swap_in访问的是不同的线性地址，映射到不同的swap分区位置(swap_nr不同)。所以不可能产生多个进程排队等待同一个swap page数据的情形。

至于两个不同的swap page倒是有可能对应相同的binary内容，比如inode A被进程1和进程2同时加载，某段相同的代码被映射到不同的线性地址空间，然后被交换到swap分区的不同位置中。

同样的道理对于swap_out，每次都会对不同的线性地址空间进行swap_out，也会写到swap分区的不同位置(swap_nr不同)

#	4.fork系统调用分析

我们分析一下典型的fork程序，如下代码：

	if ((pid=fork()) ==0) {
	   //new process 
	   child_do()
	}else{
	  //parent process 
	   parent_do()
	}

我们知道，CPU中的寄存器CS:EIP控制下一条要执行的指令。CPU会在执行当前指令的同时，将下一条即将执行的指令装载到CS:EIP中。其中CS表示代码段，EIP表示(Instruction Pointer)。形象的说就是：吃着碗里看着锅里。 所有的进程或者任务都是由CPU调度，大家共享CPU时间。 

我们可以看到，fork系统调用实际上是 
```c
static inline _syscall0(int,fork)
```
而系统调用实际上是int 0x80。

	#define _syscall0(type,name) \ 
	type name(void) \ 
	{ \ 
	long __res; \ 
	__asm__ volatile ("int $0x80" \ 
	    : "=a" (__res) \ 
	    : "0" (__NR_##name)); \ 
	if (__res >= 0) \
	    return (type) __res; \
	errno = -__res; \ 
	return -1; \ 
	} 

由此可见，在父任务中进行中断调用int 0x80时，被CPU装载进EIP的下一条即将执行的指令是 `mov %eax, _res`。fork系统调用会创建出新的一个可以被CPU调度的子任务(task_struct)，该子任务的线性地址空间为64M\*nr ~64M\*(nr+1)，同时因为使用copy-on-write技术，子任务的线性地址空间指向父任务的物理地址，并且线性地址空间的分布和父任务如出一辙。从LIB库,环境变量和参数，用户进程栈到用户程序txt-data-bss等都一致，不一样的地方一个是子任务TSS中内核态栈指针指向自己的task_struct的顶端，另一个则是子任务TSS中eax被赋值为0. 如果子任务被CPU调度到，CPU首先会从TSS中回复子任务的上下文，包括初始化所有寄存器值，(eax=0)，然后指令 
`mov %eax, _res`被执行，从而返回了0。于是可以知道当前执行的任务是新创建出的子任务，从而执行child_do()函数。这里这里子任务的代码段也指向父任务的代码段(通过页目录表和页表，页表映射到和父任务相同的物理地址)。

这里父任务代码陷入系统调用fork，引起了用户态-内核态的转换，用户态代码的CS:EIP,SS:ESP, EFLAGS等悉数被保存到父任务的内核栈上。
执行完sys_fork()函数，从中断返回，又从内核态切换到用户态，其中eax保留的是父任务的eax!=0，代码段是父任务当前线性空间上，于是父进程继而执行parent_do()函数。

    //返回的是实际物理地址 用来存放task_struct数据
	p = (struct task_struct *) get_free_page();
	if (!p)
		return -EAGAIN;
	task[nr] = p;
	*p = *current;	/* NOTE! this doesn't copy the supervisor stack */
	p->state = TASK_UNINTERRUPTIBLE;  //暂时置为不可调度，等到其他值完成以后再使其可调度
	p->pid = last_pid;
	p->counter = p->priority;
	p->signal = 0;
	p->alarm = 0;
	p->leader = 0;		/* process leadership doesn't inherit */
	p->utime = p->stime = 0;
	p->cutime = p->cstime = 0;
	p->start_time = jiffies;
	p->tss.back_link = 0;
	p->tss.esp0 = PAGE_SIZE + (long) p; //内核态栈指针设为当前task_struct所在页的顶端
	p->tss.ss0 = 0x10;  //内核代码区间
	p->tss.eip = eip;   //调用fork()的用户态进程下一条即将执行的指令 (应该是mov %eax, _res)
	p->tss.eflags = eflags;  //调用fork()的用户态进程eflags
	p->tss.eax = 0;    //注意这个是子进程得到运行机会后，从fork()系统调用中返回值。为0说明是新的子进程
	p->tss.ecx = ecx;
	p->tss.edx = edx;
	p->tss.ebx = ebx;
	p->tss.esp = esp;
	p->tss.ebp = ebp;
	p->tss.esi = esi;
	p->tss.edi = edi;
	p->tss.es = es & 0xffff;
	p->tss.cs = cs & 0xffff;
	p->tss.ss = ss & 0xffff;
	p->tss.ds = ds & 0xffff;
	p->tss.fs = fs & 0xffff;
	p->tss.gs = gs & 0xffff;


#	5.exec系统调用分析
exec调用的过程如下

*	首先会释放掉fork时从父进程拷贝过来的旧页表并将旧的线性空间地址dir置0。
*	其次会拷贝程序的环境变量和参数(这些都会驻留到主存上，并加到内存页表中)，并重新设置当前用户进程LDT的code base/limit 和data base/limit。
*	之后初始化当前进程的stack，将argc, argv \*\*, envp\*\*初始化，其中argv和envp指向后面的环境变量和参数区域。
*	最后，对当前进程的值包括executable, signal, brk, end_data, end_code, start_stack等初始化，将中断返回的eip改成新进程可执行文件的entry，将中断返回的ESP修改成当前任务的线性地址空间内的stack。

由于进程的CS,DS,SS等都已经设置为nr\*64M，从中断返回后就将执行新文件的代码。值得注意的是，将argc, argv \*\*, envp\*\*初始化时，当前stack会引发页不可用的中断，内存管理程序会分配新一页数据使用。

	0x80系统中断发生时内核栈 
	old SS 
	old ESP 
	old EFLAGS 
	old CS 
	old EIP                  —int  iret 
	DS                         — system_call 
	ES 
	FS 
	EAX
	EDX (char ** envp)
	ECX (char ** argv)
	EBX (char * filename)
	next EIP 
	old EIP address

#	6.exit系统调用分析

#	7.多任务之间的竞争

对于0.12系统而言，处理的CPU是单核的，内核代码暴露出来的主要方式是中断，所以，对于运行在内核态的代码而言，是没有race condition的。处于内核态的任务是不可以被抢占的，这一点使得内核栈不会无限的增大，最多有一个用户态-内核态切换的用户态栈保存，然后是中断调用需要保存的一些寄存器值。除非该任务在内核代码部分自己通过调用sleep将CPU时间让给别的任务。这一点是linux2.4之前非抢占式内核的一个简化设计。（Linux Kernel Development一书）。唯一的情况是从用户空间直接调用内核代码，比如读写硬盘，比如申请内存，若此时系统时钟到则该任务可能被时钟中断掉，对应的数据结构等就要考虑到这种特殊的情况，绝大多数的系统内核代码通过关闭中断cli的方式，规避掉这种race condition。典型的例子有

磁盘I/O

	ll_rw_blk.c 
	static void add_request(struct blk_dev_struct * dev, struct request * req)
	{
		struct request * tmp;

		req->next = NULL;
		//关闭IF，使得该操作不能被timer中端
		cli();
		if (req->bh)
			req->bh->b_dirt = 0;
		//当前设备没有请求，则将该请求加入，并直接执行request_fn
		if (!(tmp = dev->current_request)) {
			dev->current_request = req;
			//打开IF，下面的磁盘读写可以被timer中端
			sti();
			(dev->request_fn)();
			return;
		}
		for ( ; tmp->next ; tmp=tmp->next) {
			if (!req->bh)
				if (tmp->next->bh)
					break;
				else
					continue;
	        //如果tmp到req 或者 req到tmp->next
			if ((IN_ORDER(tmp,req) ||
			    !IN_ORDER(tmp,tmp->next)) &&
			    IN_ORDER(req,tmp->next))
				break;
		}
		//当前req放入requst链表
		req->next=tmp->next;
		tmp->next=req;
		//打开IF
		sti();
	}

内存分配

	malloc.c
	void *malloc(unsigned int len)
	{
		struct _bucket_dir	*bdir;
		struct bucket_desc	*bdesc;
		void			*retval;

		/*
		 * First we search the bucket_dir to find the right bucket change
		 * for this request.
		 */
		for (bdir = bucket_dir; bdir->size; bdir++)
			if (bdir->size >= len)
				break;
		if (!bdir->size) {
			printk("malloc called with impossibly large argument (%d)\n",
				len);
			panic("malloc: bad arg");
		}
		/*
		 * Now we search for a bucket descriptor which has free space
		 */
		cli();	/* Avoid race conditions */
		...
		sti();	/* OK, we're safe again */
	}

cli和sti中间的内核代码意味着不可以被**任意中断**“打断”，一定是以原子方式操作完成的。但是处于内核态的代码本身是可以被“打断”的，如自己主动sleep放弃对CPU的使用；时钟中断到达也会强迫内核代码进入do_timer操作，对当前任务进行计时统计；还有一种特殊的情行是80X86体系结构支撑的，内核代码访问某线性地址空间，该线性地址空间不可用从而引发CPU缺页中断，加载对应的物理内存。